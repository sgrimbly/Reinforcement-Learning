{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient (VPG) Implementation on CartPole\n",
    "https://github.com/aniket-gupta1/Reinforcement_Learning/blob/master/Policy_Optimization_methods/VPG/Iteration1_VPG.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import tensorflow as tf \n",
    "from statistics import mean\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as kls\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Initialize tensorboard object\n",
    "name = f'VPG_logs_{time.time()}'\n",
    "summary_writer = tf.summary.create_file_writer(logdir = f'logs/{name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "gamma = 0.99\n",
    "p_lr = 0.01\n",
    "v_lr = 0.01\n",
    "lam = 0.97\n",
    "train_value_iterations = 80\n",
    "num_episodes = 50\n",
    "local_steps_per_epoch = 2000\n",
    "epochs = 50\n",
    "render = False\n",
    "render_time = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "\tdef __init__(self, num_states, hidden_units, num_actions, module_name):\n",
    "\t\tsuper(Model, self).__init__() # Used to run the init method of the parent class\n",
    "\t\tself.input_layer = kl.InputLayer(input_shape = (num_states,))\n",
    "\t\tself.hidden_layers = []\n",
    "\t\tself.module_name = module_name\n",
    "\n",
    "\t\tfor hidden_unit in hidden_units:\n",
    "\t\t\tself.hidden_layers.append(kl.Dense(hidden_unit, activation = 'tanh')) # Left kernel initializer\n",
    "\t\t\n",
    "\t\tif module_name == 'policy_net':\n",
    "\t\t\tself.output_layer = kl.Dense(num_actions, activation = 'linear')\n",
    "\t\telif module_name == 'value_net':\n",
    "\t\t\tself.output_layer = kl.Dense(1, activation = 'linear')\n",
    "\n",
    "\t@tf.function\n",
    "\tdef call(self, inputs, **kwargs):\n",
    "\t\tx = self.input_layer(inputs)\n",
    "\t\tfor layer in self.hidden_layers:\n",
    "\t\t\tx = layer(x)\n",
    "\t\toutput = self.output_layer(x)\n",
    "\n",
    "\t\t# if self.module_name == 'policy_net':\n",
    "\t\t# \treturn tf.nn.log_softmax(output)\n",
    "\t\t# elif self.module_name == 'value_net':\n",
    "\t\treturn output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "\tdef __init__(self, capacity):\n",
    "\t\tself.capacity = capacity\n",
    "\t\tself.memory = []\n",
    "\t\tself.push_count = 0\n",
    "\n",
    "\tdef push(self, experience):\n",
    "\t\tif len(self.memory)<self.capacity:\n",
    "\t\t\tself.memory.append(experience)\n",
    "\t\telse:\n",
    "\t\t\tself.memory[self.push_count % self.capacity] = experience\n",
    "\t\tself.push_count += 1\n",
    "\n",
    "\tdef clear_memory(self):\n",
    "\t\tself.memory = []\n",
    "\n",
    "\tdef return_func(self, rews, discount):\n",
    "\t\tn = len(rews)\n",
    "\t\trtgs = np.zeros_like(rews, dtype = 'float32')\n",
    "\t\tfor i in reversed(range(n)):\n",
    "\t\t\trtgs[i] = rews[i] + (discount*rtgs[i+1] if i+1 < n else 0)\n",
    "\t\treturn rtgs\n",
    "\n",
    "\tdef advantage_func(self, rews, discount):\n",
    "\t\treturn scipy.signal.lfilter([1], [1, float(-discount)], rews[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\tdef __init__(self, num_actions):\n",
    "\t\tself.current_step = 0\n",
    "\t\tself.num_actions = num_actions\n",
    "\n",
    "\tdef select_action(self, state, policy_net):\n",
    "\t\treturn tf.squeeze(tf.random.categorical(policy_net(np.atleast_2d(np.atleast_2d(state).astype('float32'))), 1), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(adv):\n",
    "\tg_n = len(adv)\n",
    "\tadv = np.asarray(adv)\n",
    "\tmean = np.mean(adv)\n",
    "\tstd = np.std(adv)\n",
    "\n",
    "\treturn (adv-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode: 0 Losses:  0.01 Avg_reward:  21.74\nEpisode: 1 Losses: -0.01 Avg_reward:  47.62\nEpisode: 2 Losses: -0.00 Avg_reward:  66.67\nEpisode: 3 Losses:  0.01 Avg_reward:  95.24\nEpisode: 4 Losses: -0.02 Avg_reward:  86.96\nEpisode: 5 Losses: -0.05 Avg_reward:  117.65\nEpisode: 6 Losses:  0.02 Avg_reward:  166.67\nEpisode: 7 Losses: -0.01 Avg_reward:  142.86\nEpisode: 8 Losses: -0.04 Avg_reward:  166.67\nEpisode: 9 Losses: -0.02 Avg_reward:  181.82\nEpisode: 10 Losses: -0.05 Avg_reward:  200.00\nEpisode: 11 Losses: -0.04 Avg_reward:  200.00\nEpisode: 12 Losses: -0.01 Avg_reward:  200.00\nEpisode: 13 Losses:  0.01 Avg_reward:  200.00\nEpisode: 14 Losses:  0.02 Avg_reward:  181.82\nEpisode: 15 Losses: -0.01 Avg_reward:  200.00\nEpisode: 16 Losses:  0.03 Avg_reward:  200.00\nEpisode: 17 Losses: -0.02 Avg_reward:  181.82\nEpisode: 18 Losses: -0.00 Avg_reward:  181.82\nEpisode: 19 Losses:  0.02 Avg_reward:  200.00\nEpisode: 20 Losses:  0.01 Avg_reward:  200.00\nEpisode: 21 Losses: -0.02 Avg_reward:  181.82\nEpisode: 22 Losses:  0.07 Avg_reward:  166.67\nEpisode: 23 Losses:  0.12 Avg_reward:  153.85\nEpisode: 24 Losses:  0.16 Avg_reward:  105.26\nEpisode: 25 Losses:  0.10 Avg_reward:  74.07\nEpisode: 26 Losses:  0.25 Avg_reward:  64.52\nEpisode: 27 Losses:  0.21 Avg_reward:  62.50\nEpisode: 28 Losses:  0.10 Avg_reward:  62.50\nEpisode: 29 Losses:  0.14 Avg_reward:  58.82\nEpisode: 30 Losses: -0.03 Avg_reward:  58.82\nEpisode: 31 Losses:  0.10 Avg_reward:  60.61\nEpisode: 32 Losses:  0.33 Avg_reward:  58.82\nEpisode: 33 Losses:  0.08 Avg_reward:  68.97\nEpisode: 34 Losses:  0.08 Avg_reward:  76.92\nEpisode: 35 Losses:  0.01 Avg_reward:  86.96\nEpisode: 36 Losses:  0.02 Avg_reward:  71.43\nEpisode: 37 Losses:  0.01 Avg_reward:  76.92\nEpisode: 38 Losses: -0.00 Avg_reward:  86.96\nEpisode: 39 Losses: -0.02 Avg_reward:  66.67\nEpisode: 40 Losses: -0.00 Avg_reward:  62.50\nEpisode: 41 Losses: -0.03 Avg_reward:  51.28\nEpisode: 42 Losses: -0.03 Avg_reward:  52.63\nEpisode: 43 Losses: -0.05 Avg_reward:  66.67\nEpisode: 44 Losses: -0.05 Avg_reward:  66.67\nEpisode: 45 Losses: -0.05 Avg_reward:  86.96\nEpisode: 46 Losses: -0.06 Avg_reward:  95.24\nEpisode: 47 Losses: -0.05 Avg_reward:  111.11\nEpisode: 48 Losses: -0.07 Avg_reward:  111.11\nEpisode: 49 Losses: -0.06 Avg_reward:  125.00\n"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "array([1]) (<class 'numpy.ndarray'>) invalid",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-20a518185ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Thank you for using!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: array([1]) (<class 'numpy.ndarray'>) invalid"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Initialize Class variables\n",
    "agent = Agent(env.action_space.n)\n",
    "memory = Memory(local_steps_per_epoch)\n",
    "temp_memory = Memory(local_steps_per_epoch)\n",
    "\n",
    "# Experience tuple variable to store the experience in a defined format\n",
    "Experience = namedtuple('Experience', ['states','actions', 'rewards'])\n",
    "temp_Experience = namedtuple('Experience', ['states','actions', 'rewards', 'values'])\n",
    "\n",
    "# Initialize the policy and target network\n",
    "policy_net = Model(len(env.observation_space.sample()), [64,64], env.action_space.n, 'policy_net')\n",
    "value_net = Model(len(env.observation_space.sample()), [32], 0, 'value_net')\n",
    "\n",
    "# Optimizers for the models\n",
    "optimizer_policy_net = tf.optimizers.Adam(p_lr)\n",
    "optimizer_value_net = tf.optimizers.Adam(v_lr)\n",
    "\n",
    "# Main Loop\n",
    "for epoch in range(epochs):\n",
    "    # Reset the environment and observe the state\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    ep_rewards = []\n",
    "    returns = []\n",
    "    advantage = []\n",
    "    log_probs = []\n",
    "    avg_rewards = []\n",
    "\n",
    "    for t in range(local_steps_per_epoch):\n",
    "        if render and t%render_time == 0:\n",
    "            env.render()\n",
    "        # Select action using current policy\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        value = tf.squeeze(value_net(np.atleast_2d(np.array(state.reshape(1,-1))).astype('float32')))\n",
    "        next_state, reward, done, _ = env.step(action.numpy()[0])\n",
    "        state = next_state\n",
    "\n",
    "        # Store the data in memory for policy update\n",
    "        memory.push(Experience(state, action, reward))\n",
    "\n",
    "        \"\"\"\n",
    "        This variable is used for storing the data till the done signal is true. \n",
    "        True done signal marks the end of one episode and since we are collecting \n",
    "        multiple trajectories here, we need this variable to calculate the GAE update\n",
    "        Try to find a better approach here!\n",
    "        \"\"\"\n",
    "        temp_memory.push(temp_Experience(state, action, reward, value))\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        if done or (t+1 == local_steps_per_epoch):\n",
    "            # Compute Rewards to Go\n",
    "            returns += list(memory.return_func(ep_rewards, gamma))\n",
    "            temp = temp_Experience(*zip(*temp_memory.memory))\n",
    "\n",
    "            \"\"\"\n",
    "            This step is critical as in the last trajectory that we are collecting \n",
    "            we are not waiting for the episdoe to be over, so we need to bootstrap \n",
    "            for the value of the state\n",
    "            \"\"\"\n",
    "            last_val = 0 if done else tf.squeeze(value_net(np.atleast_2d(np.array(state.reshape(1,-1)).astype('float32'))))\n",
    "\n",
    "            temp_states, temp_actions, temp_rewards, temp_values = np.asarray(temp[0]),np.asarray(temp[1]),np.asarray(temp[2]),np.asarray(temp[3])\n",
    "            temp_values = np.append(temp_values, last_val)\n",
    "            \n",
    "            # Compute TD-target\n",
    "            delta = temp_rewards + gamma * temp_values[1:] - temp_values[:-1]\n",
    "            advantage += list(memory.advantage_func(delta, gamma*lam))\n",
    "            temp_memory.clear_memory()\n",
    "\n",
    "            avg_rewards.append(sum(ep_rewards))\n",
    "            # Reset environment to start another trajectory\n",
    "            state, done, ep_rewards = env.reset(), False, []\n",
    "\n",
    "    buf = Experience(*zip(*memory.memory))\n",
    "    states, actions, rewards = np.asarray(buf[0]),np.asarray(buf[1]),np.asarray(buf[2])\n",
    "    avg_rewards = np.mean(np.asarray(avg_rewards))\n",
    "\n",
    "    # This helps to stabilize the training of the model\n",
    "    advantage = normalize(advantage)\n",
    "\n",
    "    # Calculate the Policy and Value gradients for gradient descent\n",
    "    with tf.GradientTape() as policy_tape, tf.GradientTape() as value_tape:\n",
    "        logits = tf.nn.log_softmax(policy_net(np.atleast_2d(np.array(states)).astype('float32')))\n",
    "\n",
    "        \"\"\"\n",
    "        Since we selected only one action out of the available ones, we need\n",
    "        to identify that action using one_hot encoding\n",
    "        \"\"\"\n",
    "        one_hot_values = tf.squeeze(tf.one_hot(np.array(actions), env.action_space.n))\n",
    "        log_probs = tf.math.reduce_sum(logits * one_hot_values, axis=1)\n",
    "        policy_loss = -tf.math.reduce_mean(advantage * log_probs)\n",
    "        value_loss = kls.MSE(returns,tf.squeeze(value_net(np.atleast_2d(np.array(states)).astype('float32'))))\n",
    "\n",
    "    policy_variables = policy_net.trainable_variables\n",
    "    value_variables = value_net.trainable_variables\n",
    "    policy_gradients = policy_tape.gradient(policy_loss, policy_variables)\n",
    "    value_gradients = value_tape.gradient(value_loss, value_variables)\n",
    "\n",
    "    # Update the policy network weights using ADAM\n",
    "    optimizer_policy_net.apply_gradients(zip(policy_gradients, policy_variables))\n",
    "    \"\"\"\n",
    "    Since we know the actual rewards that we got, value loss is pretty high.\n",
    "    So we need to perform multiple iterations of gradient descent to achieve \n",
    "    a good performance\n",
    "    \"\"\"\n",
    "    for iteration in range(train_value_iterations):\n",
    "        optimizer_value_net.apply_gradients(zip(value_gradients, value_variables))\n",
    "    \n",
    "    # Book-keeping\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('Episode_returns', sum(returns), step = epoch)\n",
    "        tf.summary.scalar('Running_avg_reward', avg_rewards, step = epoch)\n",
    "        tf.summary.scalar('Losses', policy_loss, step = epoch)\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(f\"Episode: {epoch} Losses: {policy_loss: 0.2f} Avg_reward: {avg_rewards: 0.2f}\")\n",
    "\n",
    "\n",
    "# To render the environment after the training to check how the model performs.\n",
    "# You can save the weights for further use using model.save_weights() function from TF2\n",
    "render_var = input(\"Do you want to render the env(Y/N) ?\")\n",
    "if render_var == 'Y' or render_var == 'y':\n",
    "    n_render_iter = int(input(\"How many episodes? \"))\n",
    "    for i in range(n_render_iter):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state, policy_net)\n",
    "            env.render()\n",
    "            n_state, reward, done, _ = env.step(action.numpy())\n",
    "else:\n",
    "    print(\"Thank you for using!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bittf2condac87051043d914952921abff77ea10873",
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}